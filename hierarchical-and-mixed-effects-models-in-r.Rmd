---
title: "hierarchical-and-mixed-effects-models-in-r"
author: "Alexis Athens"
date: '2022-05-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(magrittr)
library(readxl)
library(lme4)
# library(broom)
library(broomExtra) # combines broom and broom.mixed

```

## Overview and Introduction to Hierarchical and Mixed Models

### What is a hierarchical model?

Use a HLM when: - data nested within itself (students nested within teachers within schools) - pool information across small sample sizes (law of large numbers, LLN, states taht as n grows, the mean gets closer to the population average) - repeated observations across groups or individuals (students across years)

```{r download-data}
# get test data
# download.file("https://assets.datacamp.com/production/repositories/1803/datasets/975fe2b0190804d854a5da90083364629fb6af2e/classroom.csv", "hierarchical-and-mixed-effects-models-in-r-data/classroom_data.csv")

student_data <- read_csv("hierarchical-and-mixed-effects-models-in-r-data/classroom_data.csv")

```

```{r plot-linear}
# Plot the data
ggplot(data = student_data, aes(x = mathknow, y = mathgain)) +
  geom_point() +
  geom_smooth(method = lm)
# no clear relationship using a linear model

# Fit a linear model
summary(lm(mathgain ~ mathknow , data =  student_data))
# confirmed statistically insignificant
```

#### Exploring multiple levels: classrooms and schools

```{r try-levels}
# Summarize the student data at the classroom level
class_data <-
  student_data %>%
  group_by(classid, schoolid) %>%
  summarize(mathgain_class = mean(mathgain),
            mathknow_class = mean(mathknow),
            n_class = n(), .groups = "keep")

# Model the math gain with the student-level data
lm(mathgain ~ mathknow, data = student_data)

# Model the math gain with the classroom-level data
lm(mathgain_class ~ mathknow_class, data = class_data)

# Summarize the data at the school level
school_data <-
  student_data %>%
  group_by(schoolid) %>%
  summarize(mathgain_school = mean(mathgain),
            mathknow_school = mean(mathknow),
            n_school = n(), .groups = 'keep')

# Model the data at the school-level
lm(mathgain_school ~ mathknow_school, data = school_data)

# Summarize school by class (s_by_c)
s_by_c_data <-
  class_data %>%
  group_by(schoolid) %>%
  summarize(mathgain_s_by_c = mean(mathgain_class),
            mathknow_s_by_c = mean(mathknow_class),
            n_s_by_c = n(), .groups = 'keep')

# Model the data at the school-level after summarizing
# students at the class level
lm(mathgain_s_by_c ~ mathknow_s_by_c, data = s_by_c_data)
```

Note that all of these produce very different estimates for the estimate of how a teacher's math knowledge impacts a student's performance gains!

### Parts of a Regression

```{r understanding-intercepts}
school_3_data <- student_data %>% filter(schoolid == 3)
  
# Use a linear model to estimate the global intercept
lm(mathgain ~ 1, data = school_3_data)

# Use summarize to calculate the mean
school_3_data %>%
    summarize(mean(mathgain))

# Use a linear model to estimate mathgain in each classroom
lm(mathgain ~ factor(classid) - 1, data = school_3_data)
# the "- 1" call estimates an intercept for each group (classid) rather than a global estimate
```

```{r understanding-slopes}

school_3_data %<>% 
  mutate(classid = as.factor(classid))

# Use a linear model to estimate how math kindergarten
# scores predict math gains later
lm(mathgain ~ mathkind, data = school_3_data)

# Build a multiple regression
lm(mathgain ~ classid + mathkind - 1, data = school_3_data)

# Build a multiple regression with interaction
lm(mathgain ~ classid * mathkind - 1, data = school_3_data)

```

### Random effects in regressions

R syntax:

lmer(y \~ x + (1 \| random_group), data = data) lmer(y \~ x + (random_slope \| random_group), data = data)

**Random-effect intercepts** Linear models in R estimate parameters that are considered fixed or non-random and are called fixed-effects. In contrast, random-effect parameters assume data share a common error distribution, and can produce different estimates when there are small amounts of data or outliers.

```{r rf-code}

student_data %<>% 
  mutate(classid = as.factor(classid))

# Build a liner model including class as fixed-effect model
lm_out <- lm(mathgain ~ classid + mathkind, data = student_data)

# Build a mixed-effect model including class id as a random-effect
lmer_out <- lmer(mathgain ~ mathkind + (1 | classid), data = student_data)

# Extract out the slope estimate for mathkind
tidy(lm_out) %>%
    filter(term == "mathkind")
    
tidy(lmer_out) %>%
    filter(term == "mathkind")

```

Notice that the std error decreases from 0.026 to .021 using the lmer model. I.e., the random effect explains more variability than the fixed effect. And there is only one estimate for classid with the lmer model, compared to one estimate for each class using the lm framework. This is because it assumes classid comes from a common distribution.

```{r rf-intercept}
# Re-run the models to load their outputs
lm_out <- lm(mathgain ~ classid + mathkind, data = student_data)
lmer_out <- lmer(mathgain ~ mathkind + (1 | classid), data = student_data)

# Add the predictions to the original data
student_data_subset <-
    student_data %>%
    mutate(lm_predict = predict(lm_out),
           lmer_predict = predict(lmer_out)) %>%
    filter(schoolid == "1")

# Plot the predicted values
ggplot(student_data_subset,
       aes(x = mathkind, y = mathgain, color = classid)) +
    geom_point() +
    geom_line(aes(x = mathkind, y = lm_predict)) +
    geom_line(aes(x = mathkind, y = lmer_predict), linetype = 'dashed') +
    xlab("Kindergarten math score") +
    ylab("Math gain later in school") +
    theme_bw() +
    scale_color_manual("Class ID", values = c("red", "blue"))
```

```{r rf-slope}
# Rescale mathkind to make the model more stable
student_data <-
	student_data %>%
    mutate(mathkind_scaled = scale(mathkind))

# Build lmer models
lmer_intercept <- lmer(mathgain ~ mathkind_scaled + (1 | classid),
                       data = student_data)
lmer_slope     <- lmer(mathgain ~ (mathkind_scaled | classid),
                       data = student_data)

# Rescale mathkind to make the model more stable
student_data <-
	student_data %>%
    mutate(mathkind_scaled = scale(mathkind))

# Re-run the models to load their outputs
lmer_intercept <- lmer(mathgain ~ mathkind_scaled + (1 | classid),
                       data = student_data)
lmer_slope     <- lmer(mathgain ~ (mathkind_scaled | classid),
                       data = student_data)

# Add the predictions to the original data
student_data_subset <-
    student_data %>%
    mutate(lmer_intercept = predict(lmer_intercept),
           lmer_slope = predict(lmer_slope)) %>%
    filter(schoolid == "1")

# Plot the predicted values
ggplot(student_data_subset,
       aes(x = mathkind_scaled, y = mathgain, color = classid)) +
    geom_point() +
    geom_line(aes(x = mathkind_scaled, y = lmer_intercept)) +
    geom_line(aes(x = mathkind_scaled, y = lmer_slope), linetype = 'dashed') +
    theme_bw() +
    scale_color_manual("Class ID", values = c("red", "blue"))
```

"The model with fixed-effect slopes has parallel lines (solid lines) because the slope estimates are the same. The model with random-effect slopes (dashed lines) does not have parallel lines because the slope estimates are different. The model with random-effect slopes (dashed lines) has lines that are shallower than the other model. This occurred because slopes are being estimated for each classroom, but include a shared distribution. This shared distribution pools information from all classrooms (including those not shown on the plot)."

```{r school-model}
# Build the model
lmer_classroom <- lmer(mathgain ~ mathknow + mathprep + sex + mathkind + ses + (1 | classid), 
    data = student_data)

# Print the model's output
print(lmer_classroom)

# Extract coefficents
lmer_coef <-
    tidy(lmer_classroom, conf.int = TRUE)

# Print coefficents
print(lmer_coef)

# Extract coefficents
lmer_coef <-
    tidy(lmer_classroom, conf.int = TRUE)

# Plot results
lmer_coef %>%
    filter(effect == "fixed" & term != "(Intercept)") %>%
    ggplot(., aes(x = term, y = estimate,
                  ymin = conf.low, ymax = conf.high)) +
    geom_hline(yintercept = 0, color = 'red') + 
    geom_point() +
    geom_linerange() +
    coord_flip() +
    theme_bw() +
    ylab("Coefficient estimate and 95% CI") +
    xlab("Regression coefficient")
## use this graph in the future!
```
